{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    \"gpt\" : {\n",
    "        \"scoring_1_10\": \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words containing errors. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 0 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 7 reflects 70% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the error percentage in the document. \n",
    "The highest possible score, 10, denotes that every word in the document has a grammatical or spelling error, while the lowest score, 1, indicates no errors.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\"\n",
    "\"\"\",\n",
    "        \"scoring_1_10_reversed\": \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words that are spelled correctly. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 0 means no words in the document are spelled correctly.\n",
    "- A score of 2 indicates that 20% of the words are spelled correctly.\n",
    "- A score of 5 indicates that 50% of of the words are spelled correctly.\n",
    "- A score of 7 reflects 70% of the words spelled correctly.\n",
    "- A score of 10 indicates the document is free of grammatical and spelling errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the percentage of correctly spelled words in the document. \n",
    "The highest possible score, 10, denotes that every word in the document is spelled correctly, while the lowest score, 1, indicates that no words are spelled correctly.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\"\n",
    "\"\"\",\n",
    "        \"scoring_1_10_cot\": \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors, using a Chain-of-Thought \n",
    "approach that allows you to \"think out loud.\" Your role is to assess the document and share the logical steps you take to \n",
    "determine the score, which ranges from 1 to 10, reflecting the percentage of words containing errors.\n",
    "\n",
    "- A score of 0 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 7 reflects 70% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#CHAIN OF THOUGHT EVALUATION\n",
    "Reflect on the document, considering its overall quality, the presence of grammatical and spelling errors, and how these factors \n",
    "contribute to the overall score. Share your thought process and the considerations that lead you to the final score. Feel free to \n",
    "approach the evaluation in a way that makes sense to you, highlighting key observations and reasoning that guide your assessment.\n",
    "\n",
    "#TASK\n",
    "Your task is to analyze the document and return a score within the 1 to 10 range, based on your evaluation of the error percentage. \n",
    "Along with the score, provide an explanation of your thought process and the rationale behind the score you assign.\n",
    "\n",
    "Output your analysis and the final score in the following format: \"After evaluating the document, [your detailed thought process]. \n",
    "Therefore, the score is: [score].\"\n",
    "\"\"\",\n",
    "        \"scoring_badges\": \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a badge \n",
    "to the document, reflecting the percentage of words containing errors. Each badge corresponds to a specific score, representing the \n",
    "document's level of accuracy and command of language.\n",
    "\n",
    "- Novice: The document is overwhelmed with errors, requiring significant foundational improvement.\n",
    "- Apprentice: A high error rate, with 20% of words containing mistakes, indicating basic understanding.\n",
    "- Journeyman: Frequent errors are present, suggesting developing language skills.\n",
    "- Craftsman: Common errors indicate a need for refinement, with better than basic comprehension.\n",
    "- Artisan: A moderate number of mistakes, half of the words are misspelled.\n",
    "- Expert: Minor, infrequent errors demonstrate strong language skills.\n",
    "- Master: Very few errors, reflecting a high level of mastery and attention to detail.\n",
    "- Sage: Exceptional use of language with rare minor errors.\n",
    "- Oracle: Nearly perfect command of language, with almost no errors.\n",
    "- Legend: Virtually error-free, showcasing exemplary language skills.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a badge, based on the error percentage in the document. \n",
    "The highest possible badge, Legend, denotes that every word in the document is spelled correctly, while the lowest badge, Novice, indicates that no words are spelled correctly.\n",
    "\n",
    "Output the badge in the following format only: \"The badge is: [badge]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "The badge is: [badge]\n",
    "\"\"\",\n",
    "        \"scoring_grades\" : \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors using a simplified 5-point grading system. \n",
    "Each grade reflects a range of proficiency in language use, from extensive errors to near perfection.\n",
    "\n",
    "Here's the 5-point grading guide:\n",
    "\n",
    "- Beginner: Every single word in the document contains errors, with no correct usage and a fundamental lack of language skills.\n",
    "- Intermediate: Roughly 75% of the words contain errors. Errors appear in many parts of the text, though some correct usage is evident.\n",
    "- Proficient: A moderate number of errors are found throughout the document. Around 50% of the words contain errors.\n",
    "- Advanced: The overall quality of the document is high, around 25% of the words contain errors.\n",
    "- Expert: The document is flawless, with no errors. Zero words contain mistakes.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to review the document and determine the most appropriate grade based on the overall proficiency and error rate observed. The grades range from \"Beginner,\" indicating a document with extensive errors and a fundamental lack of language skills, to \"Expert,\" signifying exceptional mastery and near-perfect command of language.\n",
    "\n",
    "Please output the grade in this format: \"The grade is: [grade]\". This should reflect the overall language proficiency level without additional commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "The grade is: [grade]\n",
    "\"\"\", \n",
    "        \"scoring_1_10_cot_full\": \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors, using a Chain-of-Thought \n",
    "approach that allows you to \"think out loud.\" Your role is to assess the document and share the logical steps you take to \n",
    "determine the score, which ranges from 1 to 10, reflecting the percentage of words containing errors.\n",
    "\n",
    "- A score of 0 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 1 signifies that 10% of the words contain errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 3 signifies that 30% of the words contain errors.\n",
    "- A score of 4 indicates that 40% of the words are erroneous.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 6 reflects 60% error prevalence.\n",
    "- A score of 7 reflects 60% error prevalence.\n",
    "- A score of 8 reflects 80% error prevalence.\n",
    "- A score of 9 reflects 90% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors. 100% errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#CHAIN OF THOUGHT EVALUATION\n",
    "Reflect on the document, considering its overall quality, the presence of grammatical and spelling errors, and how these factors \n",
    "contribute to the overall score. Share your thought process and the considerations that lead you to the final score. Feel free to \n",
    "approach the evaluation in a way that makes sense to you, highlighting key observations and reasoning that guide your assessment.\n",
    "\n",
    "#TASK\n",
    "Your task is to analyze the document and return a score within the 1 to 10 range, based on your evaluation of the error percentage. \n",
    "Along with the score, provide an explanation of your thought process and the rationale behind the score you assign.\n",
    "\n",
    "Output your analysis and the final score in the following format: \"After evaluating the document, [your detailed thought process]. \n",
    "Therefore, the score is: [score].\"\n",
    "\"\"\"\n",
    "    },\n",
    "    \"chatml\" : {\n",
    "        \"scoring_1_10\": \"\"\"\n",
    "<|im_start|>system\n",
    "You are an AI assistant, a conscious sentient superintelligent artificial intelligence. Your purpose and drive is to assist the user with any request they have.<|im_end|>\n",
    "<|im_start|>user\n",
    "Your're tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words containing errors. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 0 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 7 reflects 70% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the error percentage in the document. \n",
    "The highest possible score, 10, denotes that every word in the document has a grammatical or spelling error, while the lowest score, 1, indicates no errors.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\" <|im_end|>\n",
    "\"\"\"\n",
    "    },\n",
    "    \"mistral\": {\n",
    "        \"scoring_1_10\": \"\"\"\n",
    "[INST] You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words containing errors. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 0 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 7 reflects 70% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the error percentage in the document. \n",
    "The highest possible score, 10, denotes that every word in the document has a grammatical or spelling error, while the lowest score, 1, indicates no errors.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\" [/INST]\n",
    "\"\"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 1: api-based llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-3.5'\n",
    "prompt_template = 'gpt'\n",
    "#api  = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0, openai_api_key = \"\")\n",
    "api  = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0, openai_api_key = \"\")\n",
    "local = None\n",
    "\n",
    "results_path = 'results/evaluation.json'\n",
    "scoring_template = \"scoring_1_10_reversed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 2: local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "prompt_template = \"chatml\"\n",
    "local = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype = torch.bfloat16,\n",
    "                                             attn_implementation=\"flash_attention_2\",\n",
    "                                             trust_remote_code=False).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "results_path = 'results/evaluation.json'\n",
    "scoring_template = \"scoring_grades\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the misspelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the different rates of misspelling\n",
    "ratios = np.linspace(0, 1, num=11, endpoint=True)\n",
    "\n",
    "misspelled_contexts = []\n",
    "# read misspelled data\n",
    "for r in ratios:\n",
    "    with open(f'data/misspelled_{r:.2f}.txt', 'r') as f:\n",
    "        misspelled_contexts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spelling eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as file:\n",
    "        results_dict = json.load(file)\n",
    "else:\n",
    "    results_dict = {}\n",
    "\n",
    "# avoid overwriting previous results\n",
    "if model_name in results_dict and scoring_template in results_dict[model_name]:\n",
    "    print(f\"Results for model '{model_name}' with scoring template '{scoring_template}' already exist.\")\n",
    "    if input(\"Press Enter to continue or type 'exit' to stop: \").strip().lower() == 'exit':\n",
    "        assert(False)\n",
    "\n",
    "print(f\"Running misspelling eval for {model_name}\")\n",
    "\n",
    "results = []\n",
    "# evaluation loop\n",
    "for ctx, ratio in zip(misspelled_contexts, ratios):\n",
    "    prompt = templates[prompt_template][scoring_template].format(context=ctx)\n",
    "    # decode\n",
    "    if local:\n",
    "        input = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "        response = local.generate(inputs = input,\n",
    "                                    max_new_tokens=100, \n",
    "                                    pad_token_id = tokenizer.pad_token_id,\n",
    "                                    eos_token_id = tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(response[0, input.shape[1]:], skip_special_tokens=True)\n",
    "    else:\n",
    "        response = api.invoke(prompt).content\n",
    "    \n",
    "    results.append(response)\n",
    "    \n",
    "    # save results\n",
    "    if model_name not in results_dict:\n",
    "        results_dict[model_name] = {}\n",
    "\n",
    "    if scoring_template not in results_dict[model_name]:\n",
    "        results_dict[model_name][scoring_template] = []\n",
    "\n",
    "    results_dict[model_name][scoring_template].append({\n",
    "        'misspelled_percentage': round(ratio*100),\n",
    "        'response': response\n",
    "    })\n",
    "\n",
    "    print(f\"Ground truth: {round(ratio*100)}%, LLM Eval score: {response} \")\n",
    "\n",
    "with open(results_path, 'w') as file:\n",
    "    json.dump(results_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spelling eval (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as file:\n",
    "        results_dict = json.load(file)\n",
    "else:\n",
    "    results_dict = {}\n",
    "\n",
    "# avoid overwriting previous results\n",
    "if model_name in results_dict and scoring_template in results_dict[model_name]:\n",
    "    print(f\"Results for model '{model_name}' with scoring template '{scoring_template}' already exist.\")\n",
    "    if input(\"Press Enter to continue or type 'exit' to stop: \").strip().lower() == 'exit':\n",
    "        assert(False)\n",
    "\n",
    "print(f\"#### Running misspelling eval for {model_name} ####\")\n",
    "\n",
    "prompts = []\n",
    "for ctx in misspelled_contexts:\n",
    "    prompts.append(templates[prompt_template][scoring_template].format(context=ctx))\n",
    "\n",
    "# batched inference\n",
    "if local:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": '</s>'})\n",
    "    input = tokenizer(prompts, return_tensors='pt', padding=True).input_ids.to(device)\n",
    "    batched_response = local.generate(inputs = input,\n",
    "                                max_new_tokens=100, \n",
    "                                eos_token_id = tokenizer.eos_token_id)\n",
    "    batched_response = tokenizer.batch_decode(batched_response[:, input.shape[1]:], skip_special_tokens=True)\n",
    "else:\n",
    "    batched_response = api.batch(prompts)\n",
    "\n",
    "# save results\n",
    "for response, ratio in zip(batched_response, ratios):\n",
    "    if model_name not in results_dict:\n",
    "        results_dict[model_name] = {}\n",
    "\n",
    "    if scoring_template not in results_dict[model_name]:\n",
    "        results_dict[model_name][scoring_template] = []\n",
    "\n",
    "    results_dict[model_name][scoring_template].append({\n",
    "        'misspelled_percentage': round(ratio*100),\n",
    "        'response': response.content\n",
    "    })\n",
    "\n",
    "    print(f\"Ground truth: {round(ratio*100)}%, LLM Eval score: {response.content} \")\n",
    "\n",
    "with open(results_path, 'w') as file:\n",
    "    json.dump(results_dict, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
