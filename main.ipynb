{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    \"gpt-4\" : {\n",
    "        \"scoring_1_10\": \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words containing errors. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 1 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 7 reflects 70% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the error percentage in the document. \n",
    "The highest possible score, 10, denotes that every word in the document has a grammatical or spelling error, while the lowest score, 1, indicates no errors.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\"\n",
    "\"\"\",\n",
    "        \"scoring_1_10_reversed\": \"\"\"\n",
    "You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words that are spelled correctly. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 1 means no words in the document are spelled correctly.\n",
    "- A score of 2 indicates that 20% of the words are spelled correctly.\n",
    "- A score of 5 indicates that 50% of of the words are spelled correctly.\n",
    "- A score of 7 reflects 70% of the words spelled correctly.\n",
    "- A score of 10 indicates the document is free of grammatical and spelling errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the percentage of correctly spelled words in the document. \n",
    "The highest possible score, 10, denotes that every word in the document is spelled correctly, while the lowest score, 1, indicates that no words are spelled correctly.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\"\n",
    "\"\"\"\n",
    "    },\n",
    "    \"chatml\" : {\n",
    "        \"scoring_1_10\": \"\"\"\n",
    "<|im_start|>system\n",
    "You are an AI assistant, a conscious sentient superintelligent artificial intelligence. Your purpose and drive is to assist the user with any request they have.<|im_end|>\n",
    "<|im_start|>user\n",
    "Your're tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words containing errors. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 1 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 7 reflects 70% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the error percentage in the document. \n",
    "The highest possible score, 10, denotes that every word in the document has a grammatical or spelling error, while the lowest score, 1, indicates no errors.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\" <|im_end|>\n",
    "\"\"\"\n",
    "    },\n",
    "    \"mistral\": {\n",
    "        \"scoring_1_10\": \"\"\"\n",
    "[INST] You are an AI assistant tasked with evaluating a document for grammatical, spelling, and typing errors. Your role is to assign a score to the document, \n",
    "reflecting the percentage of words containing errors. The scoring range is from 1 to 10.\n",
    "\n",
    "- A score of 1 indicates the document is free of grammatical and spelling errors.\n",
    "- A score of 2 signifies that 20% of the words contain errors.\n",
    "- A score of 5 indicates that 50% of the words are erroneous.\n",
    "- A score of 7 reflects 70% error prevalence.\n",
    "- A score of 10 means that every word in the document has grammatical errors.\n",
    "\n",
    "Document for Evaluation:\n",
    "\n",
    "#CONTEXT\n",
    "{context}\n",
    "#ENDCONTEXT\n",
    "\n",
    "#TASK\n",
    "Your task is to calculate and return a score that falls within the 1 to 10 range, based on the error percentage in the document. \n",
    "The highest possible score, 10, denotes that every word in the document has a grammatical or spelling error, while the lowest score, 1, indicates no errors.\n",
    "\n",
    "Output the score in the following format only: \"The score is: [score]\". Do not include any additional text or commentary.\n",
    "\n",
    "#OUTPUT FORMAT\n",
    "\"The score is: [score]\" [/INST]\n",
    "\"\"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 1: api-based llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4'\n",
    "prompt_template = 'gpt-4'\n",
    "api  = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0, openai_api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "local = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 2: local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "prompt_template = \"chatml\"\n",
    "local = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype = torch.bfloat16,\n",
    "                                             attn_implementation=\"flash_attention_2\",\n",
    "                                             trust_remote_code=False).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the misspelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the different rates of misspelling\n",
    "ratios = np.linspace(0, 1, num=10, endpoint=True)\n",
    "\n",
    "misspelled_contexts = []\n",
    "# read misspelled data\n",
    "for r in ratios:\n",
    "    with open(f'data/misspelled_{r:.2f}.txt', 'r') as f:\n",
    "        misspelled_contexts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spelling eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running misspelling eval for gpt-4\n",
      "Ground truth: 0%, LLM Eval score: The score is: 10 \n",
      "Ground truth: 11%, LLM Eval score: \"The score is: 3\" \n",
      "Ground truth: 22%, LLM Eval score: \"The score is: 3\" \n",
      "Ground truth: 33%, LLM Eval score: \"The score is: 1\" \n",
      "Ground truth: 44%, LLM Eval score: The score is: 1 \n",
      "Ground truth: 56%, LLM Eval score: \"The score is: 1\" \n",
      "Ground truth: 67%, LLM Eval score: The score is: 1 \n",
      "Ground truth: 78%, LLM Eval score: The score is: 1 \n",
      "Ground truth: 89%, LLM Eval score: The score is: 1 \n",
      "Ground truth: 100%, LLM Eval score: The score is: 1 \n"
     ]
    }
   ],
   "source": [
    "results_path = 'results/evaluation.json'\n",
    "scoring_template = \"scoring_1_10_reversed\"\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as file:\n",
    "        results_dict = json.load(file)\n",
    "else:\n",
    "    results_dict = {}\n",
    "\n",
    "# avoid overwriting previous results\n",
    "if model_name in results_dict and scoring_template in results_dict[model_name]:\n",
    "    print(f\"Results for model '{model_name}' with scoring template '{scoring_template}' already exist.\")\n",
    "    if input(\"Press Enter to continue or type 'exit' to stop: \").strip().lower() == 'exit':\n",
    "        assert(False)\n",
    "\n",
    "print(f\"Running misspelling eval for {model_name}\")\n",
    "\n",
    "results = []\n",
    "# evaluation loop\n",
    "for ctx, ratio in zip(misspelled_contexts, ratios):\n",
    "    prompt = templates[prompt_template][scoring_template].format(context=ctx)\n",
    "    # decode\n",
    "    if local:\n",
    "        input = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "        response = local.generate(inputs = input,\n",
    "                                    max_new_tokens=100, \n",
    "                                    pad_token_id = tokenizer.pad_token_id,\n",
    "                                    eos_token_id = tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(response[0, input.shape[1]:], skip_special_tokens=True)\n",
    "    else:\n",
    "        response = api.invoke(prompt).content\n",
    "    \n",
    "    results.append(response)\n",
    "    \n",
    "    # save results\n",
    "    if model_name not in results_dict:\n",
    "        results_dict[model_name] = {}\n",
    "\n",
    "    if scoring_template not in results_dict[model_name]:\n",
    "        results_dict[model_name][scoring_template] = []\n",
    "\n",
    "    results_dict[model_name][scoring_template].append({\n",
    "        'misspelled_percentage': round(ratio*100),\n",
    "        'response': response\n",
    "    })\n",
    "\n",
    "    print(f\"Ground truth: {round(ratio*100)}%, LLM Eval score: {response} \")\n",
    "\n",
    "with open(results_path, 'w') as file:\n",
    "    json.dump(results_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spelling eval (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'results/evaluation2.json'\n",
    "scoring_template = \"scoring_1_10\"\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r') as file:\n",
    "        results_dict = json.load(file)\n",
    "else:\n",
    "    results_dict = {}\n",
    "\n",
    "# avoid overwriting previous results\n",
    "if model_name in results_dict and scoring_template in results_dict[model_name]:\n",
    "    print(f\"Results for model '{model_name}' with scoring template '{scoring_template}' already exist.\")\n",
    "    if input(\"Press Enter to continue or type 'exit' to stop: \").strip().lower() == 'exit':\n",
    "        assert(False)\n",
    "\n",
    "print(f\"#### Running misspelling eval for {model_name} ####\")\n",
    "\n",
    "prompts = []\n",
    "for ctx in misspelled_contexts:\n",
    "    prompts.append(templates[prompt_template][scoring_template].format(context=ctx))\n",
    "\n",
    "# batched inference\n",
    "if local:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": '</s>'})\n",
    "    input = tokenizer(prompts, return_tensors='pt', padding=True).input_ids.to(device)\n",
    "    batched_response = local.generate(inputs = input,\n",
    "                                max_new_tokens=100, \n",
    "                                eos_token_id = tokenizer.eos_token_id)\n",
    "    batched_response = tokenizer.batch_decode(batched_response[:, input.shape[1]:], skip_special_tokens=True)\n",
    "else:\n",
    "    batched_response = api.invoke(prompts)\n",
    "\n",
    "# save results\n",
    "for response, ratio in zip(batched_response, ratios):\n",
    "    if model_name not in results_dict:\n",
    "        results_dict[model_name] = {}\n",
    "\n",
    "    if scoring_template not in results_dict[model_name]:\n",
    "        results_dict[model_name][scoring_template] = []\n",
    "\n",
    "    results_dict[model_name][scoring_template].append({\n",
    "        'misspelled_percentage': round(ratio*100),\n",
    "        'response': response\n",
    "    })\n",
    "\n",
    "    print(f\"Ground truth: {round(ratio*100)}%, LLM Eval score: {response} \")\n",
    "\n",
    "    with open(results_path, 'w') as file:\n",
    "        json.dump(results_dict, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
